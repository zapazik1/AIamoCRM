# Техническое Задание: Модуль Базы Знаний amoCRM для AI-Помощника

## 1. Введение

### 1.1. Назначение
Данный документ описывает требования и план реализации модуля Базы Знаний (Knowledge Base, KB), предназначенного для использования AI-Помощником amoCRM (согласно основному ТЗ проекта `@Тех. задание.md`). Модуль KB будет содержать обработанную документацию и информацию по amoCRM, представленную в виде, оптимизированном для поиска и использования языковыми моделями (LLM) в рамках архитектуры Retrieval-Augmented Generation (RAG).

### 1.2. Цель
Цель создания модуля KB — предоставить AI-Помощнику доступ к актуальной, полной и структурированной информации о функционале, настройках, API и лучших практиках использования amoCRM. Это позволит Помощнику генерировать точные, релевантные и основанные на фактах ответы на запросы пользователей, касающиеся работы с системой.

### 1.3. Ключевые технологии
*   **Web Scraping:** Для автоматизированного сбора данных из веб-источников.
*   **Text Processing:** Для очистки, форматирования и сегментации (чанкинга) текста.
*   **Semantic Embeddings (OpenAI):** Для преобразования текстовых фрагментов в векторные представления, отражающие их смысл.
*   **Vector Database (ChromaDB):** Для хранения векторов и эффективного поиска семантически близких фрагментов.
*   **Retrieval-Augmented Generation (RAG):** Как основной паттерн использования KB для генерации ответов LLM (OpenAI).

## 2. Требования к Базе Знаний (KB)

### 2.1. Источники данных
*   **Обязательные:**
    *   Официальная База Знаний amoCRM: `https://www.amocrm.ru/support` (и соответствующие разделы на других языках, если применимо).
    *   Документация для разработчиков (API): `amocrm.ru/developers`.
*   **Желательные (вторичный приоритет):**
    *   Официальный блог amoCRM (статьи о функционале и кейсах).
    *   Раздел FAQ на официальном сайте.
    *   Другие надежные, авторитетные источники (например, крупные партнерские блоги с гайдами), если будут найдены.

### 2.2. Формат данных
*   **Исходный:** HTML-страницы, возможно PDF-документы.
*   **Промежуточный:** Очищенные текстовые файлы (предпочтительно Markdown для сохранения структуры: заголовки, списки, ссылки).
*   **Конечный (в векторной БД):**
    *   Текстовые чанки (фрагменты) высокого качества.
    *   Векторные представления (эмбеддинги) этих чанков, созданные моделью OpenAI.
    *   Метаданные для каждого чанка (обязательно: URL источника, желательно: заголовок статьи, заголовки разделов).

### 2.3. Стратегия Сегментации (Чанкинга)
*   **Цель:** Разбить документы на небольшие, но семантически завершенные и качественно размеченные фрагменты (чанки).
*   **Метод:** Иерархический подход с использованием структуры документа (например, Markdown заголовков).
    1.  Сначала документ делится на логические секции (например, по заголовкам H2, H3).
    2.  Затем каждая секция делится на более мелкие чанки (параграфы, группы предложений) с учетом целевого размера.
*   **Размер чанка:** Оптимальный размер определен в **250-750 токенов** (примерно 200-400 слов). Это позволяет сохранить семантическую целостность информации, не перегружая контекстное окно LLM. Каждый чанк должен быть достаточно большим, чтобы быть информативным, но достаточно маленьким для эффективного извлечения информации.
*   **Максимальный размер:** Жесткое ограничение в 1500 токенов для любого чанка, чтобы избежать слишком крупных блоков, которые могут занимать большую часть контекстного окна LLM.
*   **Перекрытие чанков (Overlap):** Использовать перекрытие между чанками (~150-200 символов или 15-20% размера чанка) для обеспечения плавного перехода контекста.
*   **Метаданные:** Каждый чанк должен иметь четкие метаданные: URL источника, заголовок документа, заголовки родительских секций, ключевые термины и бизнес-сущности.

### 2.4. Модель Векторизации (Embedding Model)
*   **Выбранная модель:** **OpenAI `text-embedding-3-small`**.
*   **Обоснование выбора:** Обеспечивает высокий баланс качества векторизации (включая русский язык и технические тексты), скорости и стоимости для задач RAG. Легко интегрируется через API OpenAI и поддерживается основными фреймворками.
*   **Требование:** Данная модель должна использоваться *единообразно* как для индексации документации, так и для векторизации запросов пользователей в реальном времени.
*   *(Альтернативы типа Google Embedding API или Open Source Sentence Transformers для MVP не используются).*

### 2.5. Векторная База Данных (Vector Database)
*   **Выбранная БД:** **`ChromaDB`**.
*   **Обоснование выбора:** Open-source, проста в установке и использовании для MVP, отлично интегрируется с Python и фреймворками типа LangChain, не требует внешних сервисов для базового использования. Идеально подходит для локальной разработки и первоначального развертывания.
*   **Требование:** Обеспечить эффективное хранение векторов OpenAI (`text-embedding-3-small`, размерность 1536) и быстрый поиск по семантической близости (ANN) с использованием метрики косинусного расстояния. Должна хранить текст чанков и их метаданные вместе с векторами.
*   *(Альтернативы типа FAISS, Pinecone, Weaviate, Qdrant для MVP не используются).*

### 2.6. Процесс Индексации
*   **Описание:** Автоматизированный процесс преобразования очищенных и сегментированных текстовых чанков в векторы с помощью `text-embedding-3-small` и их загрузка в коллекцию `ChromaDB`.
*   **Требования:**
    *   Реализовать в виде Python-скрипта.
    *   Обрабатывать все подготовленные чанки.
    *   Использовать батчинг при обращении к API OpenAI для создания эмбеддингов (учитывать rate limits).
    *   Сохранять в `ChromaDB` для каждого чанка: уникальный ID, вектор, исходный текст, метаданные.
    *   Предусмотреть обработку ошибок API и БД.
    *   Обеспечить возможность полного переиндексирования базы знаний.

### 2.7. Механизм Поиска (Retrieval)
*   **Описание:** Функция или компонент бэкенда, отвечающий за поиск релевантных чанков в `ChromaDB` по векторному представлению запроса пользователя.
*   **Требования:**
    *   Принимает на вход текст запроса пользователя и параметр `top_k` (количество возвращаемых чанков, например, 3-5).
    *   Векторизует текст запроса с использованием модели **OpenAI `text-embedding-3-small`**.
    *   Выполняет поиск `top_k` наиболее близких векторов в `ChromaDB` по косинусному расстоянию.
    *   Возвращает список найденных текстовых чанков вместе с их метаданными.
    *   Скорость поиска должна быть высокой (< 1 секунды) для обеспечения отзывчивости чата.

### 2.8. Стратегия Обновления KB
*   **MVP:** Ручной перезапуск скриптов сбора и индексации при необходимости (например, раз в месяц или после крупных обновлений amoCRM).
*   **В перспективе:** Автоматический мониторинг источников и частичное/полное обновление индекса.

### 2.9. Нефункциональные требования
*   **Производительность:** Время поиска (Retrieval) < 1 сек. Время полной индексации - приемлемо несколько часов.
*   **Масштабируемость:** `ChromaDB` должна справляться с ожидаемым объемом документации (десятки тысяч чанков).
*   **Надежность:** Устойчивость скриптов сбора/индексации к сбоям, обработка ошибок.

## 3. Пошаговый План Реализации Модуля KB

**Цель:** Получить работающий прототип KB в `ChromaDB` с векторами от `text-embedding-3-small` и функцию поиска релевантных чанков.

**Инструменты:** Python 3.11+, `requests`, `beautifulsoup4`, `markdownify`, `langchain` (или `llama-index` - для удобства работы с пайплайнами RAG), **`chromadb`**, **`openai`** (клиент для API OpenAI).

---

**Шаг 1: Сбор Исходных Данных (Data Collection)**

*   **Действие:**
    1.  Определить URL Базы Знаний (`help.amocrm.ru/hc/ru`) и Документации Разработчика (`amocrm.ru/developers`).
    2.  Написать скрипт `scraper.py` (`requests` + `BeautifulSoup`) для скачивания HTML и обхода ссылок в этих разделах.
    3.  Сохранить HTML в `data/raw_html/`.
    4.  *Предусмотреть:* Обработку ошибок, `robots.txt`, `User-Agent`, задержки.
*   **Почему:** Получаем локальные копии для обработки.
*   **Концепция:** *Web Scraping*.

---

**Шаг 2: Очистка и Преобразование Данных (Data Cleaning & Formatting)**

*   **Действие:**
    1.  Написать скрипт `parser.py` для обработки HTML из `data/raw_html/`.
    2.  Используя `BeautifulSoup`, извлечь основной контент.
    3.  **Преобразовать очищенный HTML в Markdown** (`markdownify`) для сохранения структуры.
    4.  Сохранить результат в `data/processed_markdown/`. Имя файла должно содержать информацию об источнике (URL).
*   **Почему:** Чистый Markdown - лучшая основа для качественного чанкинга и сохранения семантики.
*   **Концепция:** *HTML Parsing*, *Markdown Conversion*.

---

**Шаг 3: Сегментация Текста (Text Chunking)**

*   **Действие:**
    1.  Использовать `langchain.text_splitter.MarkdownHeaderTextSplitter` (или аналогичный рекурсивный сплиттер из LangChain/LlamaIndex) для разделения Markdown по заголовкам и далее по размеру.
    2.  Настроить сплиттер: целевой размер 250-750 токенов (~200-400 слов), перекрытие ~15%.
    3.  Установить жесткое ограничение максимального размера чанка в 1500 токенов.
    4.  Применить алгоритм объединения слишком маленьких чанков для соблюдения минимального порога информативности.
    5.  Написать скрипт `chunker.py`, читающий Markdown файлы, применяющий сплиттер.
    6.  Для каждого чанка извлекать текст и метаданные (URL, заголовки документа и секций, ключевые термины, бизнес-сущности).
    7.  Сохранить результат в формате JSONL.
*   **Почему:** Созданы качественные, семантически связанные фрагменты с метаданными для точного поиска, с оптимальным размером для RAG.
*   **Концепция:** *Structured Text Splitting*, *Chunking*, *Metadata Association*.

---

**Шаг 4: Настройка Модели Эмбеддингов (OpenAI)**

*   **Действие:**
    1.  Получить API-ключ OpenAI. Убедиться, что на аккаунте есть средства/лимиты.
    2.  Установить библиотеку `openai` (`pip install openai`).
    3.  Настроить использование модели `text-embedding-3-small`. Рекомендуется использовать обертки из `LangChain` (`OpenAIEmbeddings`) или `LlamaIndex` для удобства интеграции в пайплайн.
    4.  Обеспечить безопасное хранение API-ключа (например, через переменные окружения).
*   **Почему:** Готовим инструмент для преобразования текста в векторы.
*   **Концепция:** *Embedding API Integration*, *API Key Management*.

---

**Шаг 5: Настройка Векторной Базы Данных (ChromaDB)**

*   **Действие:**
    1.  Установить `chromadb` (`pip install chromadb`).
    2.  В скрипте индексации инициализировать клиент `ChromaDB`. Можно использовать персистентное хранилище на диске (`chromadb.PersistentClient(path="./chroma_db")`).
    3.  Создать или получить коллекцию `ChromaDB` (например, `amocrm_kb`). Рекомендуется при создании коллекции явно указать используемую модель эмбеддингов (`langchain.vectorstores.Chroma.from_documents` часто делает это автоматически, если передать объект эмбеддера OpenAI), чтобы `ChromaDB` знала размерность векторов (1536 для `text-embedding-3-small`).
*   **Почему:** Готовим хранилище для векторов и текстов.
*   **Концепция:** *Vector Store Initialization*, *Persistent Storage*, *Collection Management*.

---

**Шаг 6: Индексация Данных (Embedding & Storing in ChromaDB)**

*   **Действие:**
    1.  Написать скрипт `indexer.py`, который:
        *   Загружает чанки с метаданными (из Шага 3).
        *   Инициализирует модель эмбеддингов OpenAI (из Шага 4) и клиент `ChromaDB` (из Шага 5).
        *   Использует функции `LangChain` (`vectorstore.add_documents`) или `LlamaIndex` для итерации по чанкам, получения их векторов через API `text-embedding-3-small` (с батчингом) и добавления векторов, текстов и метаданных в коллекцию `ChromaDB`.
        *   Обрабатывает ошибки API OpenAI (rate limits, etc.) и `ChromaDB`.
*   **Почему:** Наполняем `ChromaDB` базой знаний amoCRM.
*   **Концепция:** *Vector Database Population*, *Batch Embedding*, *Error Handling*.

---

**Шаг 7: Реализация Функции Поиска (Retrieval from ChromaDB)**

*   **Действие:**
    1.  Создать функцию `retrieve_amocrm_context(query: str, top_k: int = 5) -> list[Document]` (используя тип `Document` из LangChain или аналогичный).
    2.  Внутри функции:
        *   Инициализировать модель эмбеддингов OpenAI (`text-embedding-3-small`).
        *   Инициализировать клиент `ChromaDB` и получить доступ к коллекции `amocrm_kb`.
        *   Создать объект VectorStore (`langchain.vectorstores.Chroma`) или Retriever.
        *   Использовать метод `vectorstore.similarity_search` (или `retriever.get_relevant_documents`), передав ему текст запроса `query` и `k=top_k`. LangChain/LlamaIndex автоматически векторизуют запрос с помощью указанной модели эмбеддингов и выполнят поиск в `ChromaDB`.
        *   Вернуть список найденных объектов `Document` (содержащих текст чанка и метаданные).
*   **Почему:** Создаем интерфейс для извлечения релевантной информации из KB по запросу.
*   **Концепция:** *Vector Similarity Search*, *Retriever Pattern*, *Context Retrieval*.

---

**Шаг 8: Тестирование и Итерация**

*   **Действие:**
    1.  Написать скрипт для интерактивного тестирования `retrieve_amocrm_context`.
    2.  Вводить реалистичные запросы по amoCRM.
    3.  Анализировать релевантность возвращаемых чанков и метаданных.
    4.  При необходимости:
        *   Скорректировать стратегию чанкинга (Шаг 3).
        *   Скорректировать параметр `top_k` (Шаг 7).
    5.  Перезапустить индексацию (Шаг 6) после изменений.
*   **Почему:** Убеждаемся в качестве поиска – основе RAG.
*   **Концепция:** *Relevance Evaluation*, *Parameter Tuning*.

---

**Шаг 9: Интеграция с Основным AI-Помощником**

*   **Действие:**
    1.  В коде бэкенда AI-помощника:
        *   При получении запроса от пользователя, вызвать `retrieve_amocrm_context` для получения релевантных чанков.
        *   Сформировать промпт для **OpenAI LLM (например, `gpt-4o`)**. Промпт должен включать:
            *   Четкую инструкцию (например, "Ты - AI-ассистент amoCRM. Ответь на вопрос пользователя, строго основываясь на предоставленном контексте из документации amoCRM. Цитируй источники (URL) из метаданных, если это уместно. Если контекст не содержит ответа, сообщи об этом.").
            *   Найденные чанки (текст + метаданные).
            *   Контекст из MCP (если есть).
            *   Оригинальный вопрос пользователя.
        *   Отправить промпт в API `gpt-4o`.
*   **Почему:** Замыкаем RAG-пайплайн, используя созданную KB для генерации качественных ответов.
*   **Концепция:** *RAG Pipeline*, *Prompt Engineering for RAG*, *LLM Integration*.

---

## 4. Анализ текущего состояния базы данных чанков

### 4.1 Статистика чанков

На данном этапе база знаний amoCRM включает в себя:

* **Общее количество чанков:** 916
* **Распределение по типам документации:**
  * API документация: 45.3% 
  * Документация поддержки: 29.4%
  * Обучающие материалы (tutorial): 23.0%
  * FAQ: 2.3%

### 4.2 Размер чанков

Размер чанков был оптимизирован для RAG:

* **Средний размер:** 209.3 слова / ~457.5 токенов
* **Медианный размер:** 195.5 слов
* **Распределение по размеру (слова):**
  * Очень маленькие (0-50): 0.3%
  * Маленькие (51-100): 12.0%
  * Средние (101-200): 40.9%
  * Целевые (201-400): 41.2%
  * Большие (401-600): 4.0%
  * Очень большие (601+): 1.5%

* **Распределение по токенам:**
  * Маленькие (<250): 16.3%
  * Оптимальные (250-750): 75.3%
  * Большие (751-1000): 4.8%
  * Очень большие (1001-1500): 2.6%
  * Экстремально большие (>1500): 1.0%

### 4.3 Качество метаданных

* **Ключевые термины (key_terms):** 89.6% чанков имеют извлеченные ключевые термины
* **Бизнес-сущности (entities):** 73.0% чанков имеют определенные бизнес-сущности
* **Полнота обязательных метаданных:** 100% чанков содержат исходный URL, заголовок, тип документа и раздел

### 4.4 Ключевые достижения

1. **Оптимальный размер:** Более 75% чанков находятся в идеальном для RAG диапазоне 250-750 токенов
2. **Ограничение максимального размера:** Максимальный размер чанка ограничен ~1700 токенами 
3. **Минимизация маленьких чанков:** Доля слишком маленьких чанков (< 50 слов) снижена до 0.3% (с 10.1% в первоначальной версии)
4. **Богатые метаданные:** Улучшено качество и полнота метаданных для повышения точности поиска
5. **Нулевое дублирование:** База данных не содержит точных дубликатов контента

---

**Следующие шаги:** Завершение индексации чанков в ChromaDB и реализация функций поиска для модуля RAG.
